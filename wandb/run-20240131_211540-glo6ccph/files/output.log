I moved the objects to:
[3.33606798 5.93606798]
[4.23606798 6.03606798]
I moved the objects to:
[3.03606798 7.03606798]
[3.73606798 7.63606798]
I moved the objects to:
[4.83606798 6.23606798]
[2.53606798 5.73606798]
I moved the objects to:
[4.63606798 2.83606798]
[3.53606798 6.73606798]
I moved the objects to:
[6.33606798 2.53606798]
[5.13606798 5.83606798]
Epoch 0, Loss 480.3431091308594, Accuracy 0.25
C:\Users\tomda\anaconda3\Lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
C:\Users\tomda\anaconda3\Lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Test Loss 15.715119361877441, Test Accuracy 0.4403409090909091
Epoch 1, Loss 249.6335906982422, Accuracy 0.23958333333333334
Epoch 2, Loss 71.04227447509766, Accuracy 0.17708333333333334
Epoch 3, Loss 10.867874145507812, Accuracy 0.20833333333333334
Epoch 4, Loss 2.330223321914673, Accuracy 0.25
Epoch 5, Loss 0.5451907515525818, Accuracy 0.6666666666666666
Test Loss 0.17955099046230316, Test Accuracy 0.7839330808080808
Epoch 6, Loss 1.0942599773406982, Accuracy 0.6041666666666666
Epoch 7, Loss 0.2644687592983246, Accuracy 0.8125
Epoch 8, Loss 0.31544002890586853, Accuracy 0.875
Epoch 9, Loss 0.14102889597415924, Accuracy 0.8541666666666666
Epoch 10, Loss 0.13768316805362701, Accuracy 0.8958333333333334
Test Loss 0.11534389853477478, Test Accuracy 0.9940025252525254
Epoch 11, Loss 0.26291623711586, Accuracy 0.84375
Epoch 12, Loss 0.3123379945755005, Accuracy 0.8333333333333334
Epoch 13, Loss 0.2779140770435333, Accuracy 0.8333333333333334
Epoch 14, Loss 0.1905704289674759, Accuracy 0.8541666666666666
Epoch 15, Loss 0.23892515897750854, Accuracy 0.8541666666666666
Test Loss 0.044317349791526794, Test Accuracy 0.876736111111111
Epoch 16, Loss 0.30362704396247864, Accuracy 0.8333333333333334
Epoch 17, Loss 0.1050562933087349, Accuracy 0.8229166666666666
Epoch 18, Loss 0.2808784544467926, Accuracy 0.84375
Epoch 19, Loss 0.1417924016714096, Accuracy 0.8645833333333334
Epoch 20, Loss 0.1936272531747818, Accuracy 0.875
Test Loss 0.4482403099536896, Test Accuracy 0.9545454545454546
Epoch 21, Loss 1.069151520729065, Accuracy 0.75
Epoch 22, Loss 0.3476036787033081, Accuracy 0.8541666666666666
Epoch 23, Loss 0.07242552191019058, Accuracy 0.8645833333333334
Epoch 24, Loss 0.19814397394657135, Accuracy 0.8854166666666666
Epoch 25, Loss 0.25368472933769226, Accuracy 0.8541666666666666
Test Loss 0.18589264154434204, Test Accuracy 0.9771148989898989
Epoch 26, Loss 0.22011041641235352, Accuracy 0.8854166666666666
Epoch 27, Loss 0.18324868381023407, Accuracy 0.8854166666666666
Epoch 28, Loss 0.16599176824092865, Accuracy 0.8854166666666666
Epoch 29, Loss 0.32580775022506714, Accuracy 0.8854166666666666
Epoch 30, Loss 0.1303672045469284, Accuracy 0.8958333333333334
Test Loss 0.007098671514540911, Test Accuracy 1.0711805555555556
Epoch 31, Loss 0.14585529267787933, Accuracy 0.8958333333333334
Epoch 32, Loss 0.08593183755874634, Accuracy 0.8958333333333334
Epoch 33, Loss 0.20868496596813202, Accuracy 0.9270833333333334
Epoch 34, Loss 0.20343613624572754, Accuracy 0.9270833333333334
Epoch 35, Loss 0.11929380893707275, Accuracy 0.9270833333333334
Test Loss 0.03487076237797737, Test Accuracy 1.045138888888889
Epoch 36, Loss 0.029392017051577568, Accuracy 0.9479166666666666
Epoch 37, Loss 0.05401776358485222, Accuracy 0.9375
Epoch 38, Loss 0.12861178815364838, Accuracy 0.9270833333333334
Traceback (most recent call last):
  File "C:\Users\tomda\Desktop\symmetry_learning\symmetry_shift_runner.py", line 51, in <module>
    main(config)
  File "C:\Users\tomda\Desktop\symmetry_learning\symmetry_shift_runner.py", line 36, in main
    bc_control(config)
  File "C:\Users\tomda\Desktop\symmetry_learning\symmetry_shift_runner.py", line 17, in bc_control
    bcRunner(env)
  File "C:\Users\tomda\Desktop\symmetry_learning\algos\bc.py", line 279, in bcRunner
    bc(env, model)
  File "C:\Users\tomda\Desktop\symmetry_learning\algos\bc.py", line 175, in bc
    outputs = model(states)
              ^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\Desktop\symmetry_learning\algos\bc.py", line 74, in forward
    x = self.pool(F.relu(self.conv1(x)))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\torch\nn\modules\pooling.py", line 166, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\torch\_jit_internal.py", line 488, in fn
    return if_false(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\torch\nn\functional.py", line 791, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Epoch 39, Loss 0.14207057654857635, Accuracy 0.9375
Epoch 40, Loss 0.20320850610733032, Accuracy 0.9479166666666666
Test Loss 0.07633358240127563, Test Accuracy 1.0538194444444444
Epoch 41, Loss 0.07038766890764236, Accuracy 0.9270833333333334