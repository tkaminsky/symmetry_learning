I moved the objects to:
[7.00277564 7.90277564]
[5.60277564 3.60277564]
I moved the objects to:
[4.90277564 7.70277564]
[7.80277564 2.80277564]
I moved the objects to:
[7.20277564 6.40277564]
[6.20277564 6.60277564]
I moved the objects to:
[2.90277564 7.90277564]
[2.30277564 5.90277564]
I moved the objects to:
[7.90277564 3.20277564]
[7.40277564 4.60277564]
Epoch 0, Loss 61.54072189331055, Accuracy 0.3118872549019608
C:\Users\tomda\anaconda3\Lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
C:\Users\tomda\anaconda3\Lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Test Loss 295.96087646484375, Test Accuracy 0.31229575163398693
Epoch 1, Loss 47.18210983276367, Accuracy 0.400735294117647
Epoch 2, Loss 9.007586479187012, Accuracy 0.5119485294117647
Epoch 3, Loss 0.7036619782447815, Accuracy 0.5337009803921569
Epoch 4, Loss 0.01110453438013792, Accuracy 0.875
Epoch 5, Loss 0.14851386845111847, Accuracy 0.870404411764706
Test Loss 10.931205749511719, Test Accuracy 0.5818014705882354
Epoch 6, Loss 0.33800482749938965, Accuracy 0.7564338235294118
Epoch 7, Loss 0.0257987380027771, Accuracy 0.9427083333333334
Epoch 8, Loss 0.0003117510350421071, Accuracy 0.9375
Epoch 9, Loss 0.251850962638855, Accuracy 0.9589460784313726
Epoch 10, Loss 0.000494212843477726, Accuracy 0.984375
Test Loss 6.057710647583008, Test Accuracy 0.7552083333333334
Epoch 11, Loss 0.04533601924777031, Accuracy 0.9543504901960785
Epoch 12, Loss 0.31871387362480164, Accuracy 0.917279411764706
Epoch 13, Loss 0.0006318982341326773, Accuracy 0.9739583333333334
Epoch 14, Loss 4.6650140575366095e-05, Accuracy 0.9895833333333334
Epoch 15, Loss 0.0003452943929005414, Accuracy 0.9947916666666666
Test Loss 1.3746625185012817, Test Accuracy 0.7378472222222222
Epoch 16, Loss 0.03880394622683525, Accuracy 0.9745710784313726
Epoch 17, Loss 0.019697314128279686, Accuracy 0.9745710784313726
Epoch 18, Loss 0.012244022451341152, Accuracy 0.9947916666666666
Epoch 19, Loss 0.0014602196170017123, Accuracy 0.9947916666666666
Epoch 20, Loss 0.005079828668385744, Accuracy 0.9947916666666666
Test Loss 0.0032437844201922417, Test Accuracy 0.9357638888888888
Epoch 21, Loss 0.28277042508125305, Accuracy 0.8131127450980392
Epoch 22, Loss 0.018017077818512917, Accuracy 0.8964460784313726
Epoch 23, Loss 0.020621374249458313, Accuracy 0.9745710784313726
Epoch 24, Loss 0.019191088154911995, Accuracy 0.9791666666666666
Epoch 25, Loss 0.0003497629368212074, Accuracy 0.9895833333333334
Test Loss 1.3968886137008667, Test Accuracy 0.9861111111111112
Epoch 26, Loss 0.0071721612475812435, Accuracy 0.9947916666666666
Epoch 27, Loss 0.0023355958983302116, Accuracy 1.0
Epoch 28, Loss 0.00210479530505836, Accuracy 0.9947916666666666
Epoch 29, Loss 5.073248394182883e-05, Accuracy 0.9947916666666666
Epoch 30, Loss 4.0676026401342824e-05, Accuracy 1.0
Test Loss 0.5194627642631531, Test Accuracy 0.875
Epoch 31, Loss 0.010362940840423107, Accuracy 1.0
Epoch 32, Loss 0.0004895760794170201, Accuracy 0.9947916666666666
Epoch 33, Loss 0.012522384524345398, Accuracy 1.0
Epoch 34, Loss 0.00022944103693589568, Accuracy 1.0
Epoch 35, Loss 0.0006421033176593482, Accuracy 1.0
Test Loss 0.7107447981834412, Test Accuracy 1.0625
Epoch 36, Loss 0.00012021631846437231, Accuracy 1.0
Epoch 37, Loss 0.0015079554868862033, Accuracy 0.9947916666666666
Epoch 38, Loss 0.003824867308139801, Accuracy 0.9947916666666666
Epoch 39, Loss 0.006929910276085138, Accuracy 1.0
Epoch 40, Loss 0.0001911848085001111, Accuracy 1.0
Test Loss 0.000483582349261269, Test Accuracy 1.1979166666666667
Epoch 41, Loss 0.004777876194566488, Accuracy 1.0
Epoch 42, Loss 0.0003437293053139001, Accuracy 1.0
Epoch 43, Loss 0.0016657068626955152, Accuracy 1.0
Epoch 44, Loss 0.0007306893239729106, Accuracy 1.0
Epoch 45, Loss 0.0012732722098007798, Accuracy 1.0
Test Loss 0.8377172350883484, Test Accuracy 1.03125
Epoch 46, Loss 0.0003644227108452469, Accuracy 1.0
Epoch 47, Loss 0.00019961617363151163, Accuracy 1.0
Epoch 48, Loss 0.0001753801916493103, Accuracy 1.0
Epoch 49, Loss 0.0020662855822592974, Accuracy 1.0
Epoch 50, Loss 7.040777563815936e-05, Accuracy 1.0
Test Loss 0.0005800293292850256, Test Accuracy 1.1979166666666667
Epoch 51, Loss 0.0004614531935658306, Accuracy 1.0
Epoch 52, Loss 0.00020697753643617034, Accuracy 1.0
Epoch 53, Loss 0.00011994671513093635, Accuracy 1.0
Epoch 54, Loss 0.00034062264603562653, Accuracy 1.0
Epoch 55, Loss 0.0003390950441826135, Accuracy 1.0
Test Loss 0.00011692380212480202, Test Accuracy 1.1770833333333333
Epoch 56, Loss 0.0001412040292052552, Accuracy 1.0
Epoch 57, Loss 0.0001674915401963517, Accuracy 1.0
Epoch 58, Loss 0.00013157051580492407, Accuracy 1.0
Epoch 59, Loss 0.00015041882579680532, Accuracy 1.0
Epoch 60, Loss 0.0006613619043491781, Accuracy 1.0
Test Loss 3.11953067779541, Test Accuracy 1.0729166666666667
Epoch 61, Loss 0.0007454051519744098, Accuracy 1.0
Epoch 62, Loss 0.00011762209032895043, Accuracy 1.0
Epoch 63, Loss 3.2768679375294596e-05, Accuracy 1.0
Epoch 64, Loss 5.939719267189503e-05, Accuracy 1.0
Epoch 65, Loss 0.00016729084018152207, Accuracy 1.0
Test Loss 1.9868208767093165e-07, Test Accuracy 1.2083333333333333
Epoch 66, Loss 0.0011704465141519904, Accuracy 1.0
Epoch 67, Loss 0.00013047133688814938, Accuracy 1.0
Epoch 68, Loss 4.975916453986429e-05, Accuracy 1.0
Epoch 69, Loss 6.655872130068019e-05, Accuracy 1.0
Epoch 70, Loss 0.0002650737587828189, Accuracy 1.0
Test Loss 0.9788126945495605, Test Accuracy 1.09375
Epoch 71, Loss 0.000267008290393278, Accuracy 1.0
Epoch 72, Loss 0.00030341450474224985, Accuracy 1.0
Epoch 73, Loss 0.0005988331395201385, Accuracy 1.0
Epoch 74, Loss 0.0002709648397285491, Accuracy 1.0
Epoch 75, Loss 0.00023146734747570008, Accuracy 1.0
Test Loss 0.0004809802630916238, Test Accuracy 1.25
Epoch 76, Loss 8.009158773347735e-05, Accuracy 1.0
Epoch 77, Loss 0.0005975871463306248, Accuracy 1.0
Epoch 78, Loss 7.827369699953124e-05, Accuracy 1.0
Epoch 79, Loss 0.0001415331644238904, Accuracy 1.0
Epoch 80, Loss 0.00016331340884789824, Accuracy 1.0
Test Loss 0.2153807431459427, Test Accuracy 1.2916666666666667
Epoch 81, Loss 0.0005676238215528429, Accuracy 1.0
Epoch 82, Loss 0.00026463650283403695, Accuracy 1.0
Epoch 83, Loss 0.00040554977022111416, Accuracy 1.0
Epoch 84, Loss 4.945799082634039e-05, Accuracy 1.0
Epoch 85, Loss 2.870559001166839e-05, Accuracy 1.0
Test Loss 0.13858234882354736, Test Accuracy 1.2916666666666667
Epoch 86, Loss 0.00010131509770872071, Accuracy 1.0
Epoch 87, Loss 5.578158379648812e-05, Accuracy 1.0
Epoch 88, Loss 4.164121128269471e-05, Accuracy 1.0
Epoch 89, Loss 0.00011815266771009192, Accuracy 1.0
Epoch 90, Loss 3.0768926080781966e-05, Accuracy 1.0
Test Loss 0.13378480076789856, Test Accuracy 1.2604166666666667
Epoch 91, Loss 9.780587424756959e-05, Accuracy 1.0
Epoch 92, Loss 0.0011872990289703012, Accuracy 1.0
Epoch 93, Loss 8.973491640063003e-05, Accuracy 1.0
Epoch 94, Loss 0.0004949205904267728, Accuracy 1.0
Epoch 95, Loss 3.32419149344787e-05, Accuracy 1.0
Test Loss 0.0007969738799147308, Test Accuracy 1.1770833333333333
Epoch 96, Loss 1.4798941265325993e-05, Accuracy 0.9947916666666666
Epoch 97, Loss 0.025758445262908936, Accuracy 0.9849877450980392
Epoch 98, Loss 0.0018660164205357432, Accuracy 1.0
Epoch 99, Loss 7.017487223492935e-05, Accuracy 1.0
Traceback (most recent call last):
  File "C:\Users\tomda\Desktop\symmetry_learning\symmetry_shift_runner.py", line 51, in <module>
    main(config)
  File "C:\Users\tomda\Desktop\symmetry_learning\symmetry_shift_runner.py", line 36, in main
    bc_control(config)
  File "C:\Users\tomda\Desktop\symmetry_learning\symmetry_shift_runner.py", line 17, in bc_control
    bcRunner(env)
  File "C:\Users\tomda\Desktop\symmetry_learning\algos\bc.py", line 279, in bcRunner
    bc(env, model)
  File "C:\Users\tomda\Desktop\symmetry_learning\algos\bc.py", line 259, in bc
    axs[0].plot(losses)
  File "C:\Users\tomda\anaconda3\Lib\site-packages\matplotlib\axes\_axes.py", line 1688, in plot
    lines = [*self._get_lines(*args, data=data, **kwargs)]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\matplotlib\axes\_base.py", line 311, in __call__
    yield from self._plot_args(
               ^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\matplotlib\axes\_base.py", line 496, in _plot_args
    x, y = index_of(xy[-1])
           ^^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\matplotlib\cbook\__init__.py", line 1661, in index_of
    y = _check_1d(y)
        ^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\matplotlib\cbook\__init__.py", line 1353, in _check_1d
    return np.atleast_1d(x)
           ^^^^^^^^^^^^^^^^
  File "<__array_function__ internals>", line 200, in atleast_1d
  File "C:\Users\tomda\anaconda3\Lib\site-packages\numpy\core\shape_base.py", line 65, in atleast_1d
    ary = asanyarray(ary)
          ^^^^^^^^^^^^^^^
  File "C:\Users\tomda\anaconda3\Lib\site-packages\torch\_tensor.py", line 1030, in __array__
    return self.numpy()
           ^^^^^^^^^^^^
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.